{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f9a9170",
   "metadata": {},
   "source": [
    "# Praktikum 4: Pengenalan Tensor pada PyTorch\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/pakizhan-ump/ml-umpontianak/blob/main/Modules/Week-02/Praktikum-02/Praktikum_4_tensor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "## 🎯 Tujuan Praktikum\n",
    "Mahasiswa memahami struktur data dasar dalam library **PyTorch**, yaitu **Tensor**, dan mampu melakukan operasi-operasi fundamental yang menjadi dasar dari komputasi deep learning.\n",
    "\n",
    "## 📖 Dasar Teori\n",
    "**PyTorch** adalah salah satu *framework* deep learning terpopuler. Unit data fundamental di PyTorch adalah **Tensor**. Secara konseptual, sebuah tensor adalah generalisasi dari vektor dan matriks ke dimensi yang lebih tinggi.\n",
    "* Skalar = Tensor 0D\n",
    "* Vektor = Tensor 1D\n",
    "* Matriks = Tensor 2D\n",
    "\n",
    "Tensor sangat mirip dengan NumPy ndarray, namun memiliki dua keunggulan krusial untuk deep learning:\n",
    "1.  **Akselerasi GPU:** Tensor dapat dengan mudah dipindahkan ke *Graphics Processing Unit* (GPU) untuk mendapatkan percepatan komputasi yang masif. Ini sangat penting untuk melatih model deep learning yang kompleks pada data besar.\n",
    "2.  **Dukungan Autograd:** PyTorch dapat secara otomatis melacak histori operasi pada tensor dan menghitung gradien (turunan) dari operasi tersebut. Fitur ini, yang disebut *automatic differentiation*, adalah inti dari algoritma *backpropagation* yang digunakan untuk melatih hampir semua jaringan saraf tiruan.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e6d78a",
   "metadata": {},
   "source": [
    "# 🔧 OPERASI FUNDAMENTAL PYTORCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "252f9628",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "450749ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c16a5029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d9085c",
   "metadata": {},
   "source": [
    "# 1. TENSOR CREATION & TYPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23f7e58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TENSOR CREATION ===\n",
      "Zeros tensor:\n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "Random tensor:\n",
      " tensor([[0.2563, 0.1942, 0.5408],\n",
      "        [0.6227, 0.8988, 0.5938]])\n",
      "Arange tensor: tensor([0, 2, 4, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "print(\"=== TENSOR CREATION ===\")\n",
    "# Various creation methods\n",
    "tensor_zeros = torch.zeros(2, 3)                    # Zeros tensor\n",
    "tensor_ones = torch.ones(2, 3)                      # Ones tensor  \n",
    "tensor_rand = torch.rand(2, 3)                      # Random [0,1)\n",
    "tensor_randn = torch.randn(2, 3)                    # Normal distribution\n",
    "tensor_arange = torch.arange(0, 10, 2)              # Like range()\n",
    "\n",
    "print(\"Zeros tensor:\\n\", tensor_zeros)\n",
    "print(\"Random tensor:\\n\", tensor_rand)\n",
    "print(\"Arange tensor:\", tensor_arange)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323fa6d4",
   "metadata": {},
   "source": [
    "# 2. DATA TYPES & SHAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c58aa0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DATA TYPES & SHAPE ===\n",
      "Integer tensor dtype: torch.int32\n",
      "Float tensor dtype: torch.float32\n",
      "Tensor shape: torch.Size([3])\n",
      "Tensor size: torch.Size([3])\n",
      "Number of elements: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== DATA TYPES & SHAPE ===\")\n",
    "tensor_int = torch.tensor([1, 2, 3], dtype=torch.int32)\n",
    "tensor_float = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32)\n",
    "\n",
    "print(\"Integer tensor dtype:\", tensor_int.dtype)\n",
    "print(\"Float tensor dtype:\", tensor_float.dtype)\n",
    "print(\"Tensor shape:\", tensor_float.shape)\n",
    "print(\"Tensor size:\", tensor_float.size())\n",
    "print(\"Number of elements:\", tensor_float.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54d872a",
   "metadata": {},
   "source": [
    "# 3. ACCESS & SLICING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38400ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ACCESS & SLICING ===\n",
      "Original tensor:\n",
      " tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "Element [1,2]: tensor(6)\n",
      "First row: tensor([1, 2, 3])\n",
      "Last column: tensor([3, 6, 9])\n",
      "Submatrix:\n",
      " tensor([[2, 3],\n",
      "        [5, 6]])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== ACCESS & SLICING ===\")\n",
    "tensor_2d = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(\"Original tensor:\\n\", tensor_2d)\n",
    "print(\"Element [1,2]:\", tensor_2d[1, 2])\n",
    "print(\"First row:\", tensor_2d[0, :])\n",
    "print(\"Last column:\", tensor_2d[:, -1])\n",
    "print(\"Submatrix:\\n\", tensor_2d[0:2, 1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed427bea",
   "metadata": {},
   "source": [
    "# 4. RESHAPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35df00a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RESHAPING ===\n",
      "Original shape: torch.Size([12])\n",
      "Reshaped 3x4:\n",
      " tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "Transposed 4x3:\n",
      " tensor([[ 0,  4,  8],\n",
      "        [ 1,  5,  9],\n",
      "        [ 2,  6, 10],\n",
      "        [ 3,  7, 11]])\n",
      "Flattened: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== RESHAPING ===\")\n",
    "tensor_1d = torch.arange(12)\n",
    "print(\"Original shape:\", tensor_1d.shape)\n",
    "\n",
    "reshaped = tensor_1d.reshape(3, 4)                  # Reshape\n",
    "viewed = tensor_1d.view(3, 4)                       # View (shares memory)\n",
    "transposed = reshaped.T                             # Transpose\n",
    "flattened = reshaped.flatten()                      # Flatten\n",
    "\n",
    "print(\"Reshaped 3x4:\\n\", reshaped)\n",
    "print(\"Transposed 4x3:\\n\", transposed)\n",
    "print(\"Flattened:\", flattened)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f24385",
   "metadata": {},
   "source": [
    "# 5. MATHEMATICAL OPERATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b41deab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MATHEMATICAL OPERATIONS ===\n",
      "a + b = tensor([5., 7., 9.])\n",
      "a * b = tensor([ 4., 10., 18.])\n",
      "a ** 2 = tensor([1., 4., 9.])\n",
      "torch.matmul: tensor(32.)\n",
      "torch.sum(a) = tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== MATHEMATICAL OPERATIONS ===\")\n",
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "b = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "print(\"a + b =\", a + b)                             # Element-wise\n",
    "print(\"a * b =\", a * b)\n",
    "print(\"a ** 2 =\", a ** 2)\n",
    "print(\"torch.matmul:\", torch.matmul(a, b))          # Dot product\n",
    "print(\"torch.sum(a) =\", torch.sum(a))               # Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d91679",
   "metadata": {},
   "source": [
    "# 6. AGGREGATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbed82c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AGGREGATION ===\n",
      "Data tensor:\n",
      " tensor([[ 2.1043,  0.0182,  0.8012, -0.6319, -0.9294],\n",
      "        [ 0.3696,  0.1031,  0.6994,  0.8108, -0.6064],\n",
      "        [-0.7120, -0.2122, -0.5201, -0.3885,  1.4179],\n",
      "        [-0.2241,  0.6552, -0.6698,  0.3290, -2.0579]])\n",
      "Global sum: tensor(0.3562)\n",
      "Mean along dim 0: tensor([ 0.3844,  0.1411,  0.0777,  0.0298, -0.5440])\n",
      "Max along dim 1: torch.return_types.max(\n",
      "values=tensor([2.1043, 0.8108, 1.4179, 0.6552]),\n",
      "indices=tensor([0, 3, 4, 1]))\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== AGGREGATION ===\")\n",
    "data = torch.randn(4, 5)                            # Random data\n",
    "print(\"Data tensor:\\n\", data)\n",
    "print(\"Global sum:\", torch.sum(data))\n",
    "print(\"Mean along dim 0:\", torch.mean(data, dim=0)) # Column means\n",
    "print(\"Max along dim 1:\", torch.max(data, dim=1))   # Row maximums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aea4afc",
   "metadata": {},
   "source": [
    "# 7. BROADCASTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d02b4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BROADCASTING ===\n",
      "Matrix:\n",
      " tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "Vector: tensor([10, 20, 30])\n",
      "Broadcast result:\n",
      " tensor([[11, 22, 33],\n",
      "        [14, 25, 36]])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== BROADCASTING ===\")\n",
    "matrix = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "vector = torch.tensor([10, 20, 30])\n",
    "\n",
    "result = matrix + vector                            # Broadcasting\n",
    "print(\"Matrix:\\n\", matrix)\n",
    "print(\"Vector:\", vector)\n",
    "print(\"Broadcast result:\\n\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f36b977",
   "metadata": {},
   "source": [
    "# 8. RANDOM OPERATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "114efba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RANDOM OPERATIONS ===\n",
      "Uniform random:\n",
      " tensor([[0.8823, 0.9150, 0.3829],\n",
      "        [0.9593, 0.3904, 0.6009]])\n",
      "Normal random:\n",
      " tensor([[ 1.1561,  0.3965, -2.4661],\n",
      "        [ 0.3623,  0.3765, -0.1808]])\n",
      "Random integers:\n",
      " tensor([[7, 6, 9],\n",
      "        [6, 3, 1]])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== RANDOM OPERATIONS ===\")\n",
    "torch.manual_seed(42)                               # Set seed\n",
    "random_tensor = torch.rand(2, 3)                    # Uniform\n",
    "normal_tensor = torch.randn(2, 3)                   # Normal\n",
    "randint_tensor = torch.randint(0, 10, (2, 3))       # Integers\n",
    "\n",
    "print(\"Uniform random:\\n\", random_tensor)\n",
    "print(\"Normal random:\\n\", normal_tensor)\n",
    "print(\"Random integers:\\n\", randint_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a8da42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. CONCATENATION & SPLITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5bfc17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CONCATENATION & SPLITTING ===\n",
      "Vertical concat:\n",
      " tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6],\n",
      "        [7, 8]])\n",
      "Horizontal concat:\n",
      " tensor([[1, 2, 5, 6],\n",
      "        [3, 4, 7, 8]])\n",
      "After splitting:\n",
      "Chunk 0:\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "Chunk 1:\n",
      "tensor([[5, 6],\n",
      "        [7, 8]])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== CONCATENATION & SPLITTING ===\")\n",
    "t1 = torch.tensor([[1, 2], [3, 4]])\n",
    "t2 = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "# Concatenation\n",
    "cat_vertical = torch.cat([t1, t2], dim=0)           # Vertical\n",
    "cat_horizontal = torch.cat([t1, t2], dim=1)         # Horizontal\n",
    "\n",
    "print(\"Vertical concat:\\n\", cat_vertical)\n",
    "print(\"Horizontal concat:\\n\", cat_horizontal)\n",
    "\n",
    "# Splitting\n",
    "chunks = torch.chunk(cat_vertical, 2, dim=0)        # Split into chunks\n",
    "print(\"After splitting:\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i}:\\n{chunk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "297dad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. GPU OPERATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "496b3b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPU OPERATIONS ===\n",
      "GPU not available, using CPU\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== GPU OPERATIONS ===\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    tensor_gpu = tensor_2d.to(device)               # Move to GPU\n",
    "    print(f\"Tensor on: {tensor_gpu.device}\")\n",
    "    \n",
    "    # Operations on GPU\n",
    "    result_gpu = tensor_gpu + 1\n",
    "    print(\"GPU operation result on CPU:\", result_gpu.cpu())\n",
    "else:\n",
    "    print(\"GPU not available, using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29e1f4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🏋️ LATIHAN 4: OPERASI PYTORCH UNTUK DEEP LEARNING\n",
    "\n",
    "### TENSOR OPERATIONS FOR NEURAL NETWORKS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61ddec89",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     35\u001b[39m labels = torch.randint(\u001b[32m0\u001b[39m, \u001b[32m3\u001b[39m, (\u001b[32m10\u001b[39m,))\n\u001b[32m     36\u001b[39m one_hot = one_hot_pytorch(labels, num_classes=\u001b[32m3\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m == (batch_size, \u001b[32m1\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mLinear output shape incorrect\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m torch.all(activated >= \u001b[32m0\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mReLU should be >= 0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m normalized.shape == X.shape, \u001b[33m\"\u001b[39m\u001b[33mBatch norm should preserve shape\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "\n",
    "'''TODO: Implementasi Operasi Dasar Neural Networks'''\n",
    "# Simulasi batch data: 32 samples, 10 features\n",
    "batch_size, n_features = 32, 10\n",
    "X = torch.randn(batch_size, n_features)\n",
    "weights = torch.randn(n_features, 1)\n",
    "bias = torch.randn(1)\n",
    "\n",
    "# TODO 1: Implementasi linear layer manual: y = XW + b\n",
    "def linear_layer(X, W, b):\n",
    "    # TODO: Implementasi operasi linear\n",
    "    pass\n",
    "\n",
    "output = linear_layer(X, weights, bias)\n",
    "\n",
    "# TODO 2: Implementasi ReLU activation function\n",
    "def relu_activation(tensor):\n",
    "    # TODO: Implementasi ReLU: max(0, x)\n",
    "    pass\n",
    "\n",
    "activated = relu_activation(output)\n",
    "\n",
    "# TODO 3: Batch normalization sederhana\n",
    "def simple_batch_norm(tensor):\n",
    "    # TODO: Normalisasi per feature across batch\n",
    "    # Formula: (x - mean) / (std + epsilon)\n",
    "    pass\n",
    "\n",
    "normalized = simple_batch_norm(X)\n",
    "\n",
    "# TODO 4: One-hot encoding manual\n",
    "def one_hot_pytorch(labels, num_classes):\n",
    "    # TODO: Convert labels to one-hot encoding\n",
    "    pass\n",
    "\n",
    "labels = torch.randint(0, 3, (10,))\n",
    "one_hot = one_hot_pytorch(labels, num_classes=3)\n",
    "\n",
    "assert output.shape == (batch_size, 1), \"Linear output shape incorrect\"\n",
    "assert torch.all(activated >= 0), \"ReLU should be >= 0\"\n",
    "assert normalized.shape == X.shape, \"Batch norm should preserve shape\"\n",
    "assert one_hot.shape == (10, 3), \"One-hot shape incorrect\"\n",
    "print(\"✅ PyTorch operations completed\")\n",
    "\n",
    "### BONUS: ADVANCED TENSOR OPERATIONS ###\n",
    "\n",
    "'''TODO: Matrix Multiplication dari Prinsip Dasar'''\n",
    "def manual_matrix_multiply(A, B):\n",
    "    \"\"\"\n",
    "    Implementasi perkalian matriks manual tanpa torch.matmul\n",
    "    \"\"\"\n",
    "    # TODO: Implementasi perkalian matriks dari dasar\n",
    "    # Hint: Gunakan nested loops atau broadcasting\n",
    "    pass\n",
    "\n",
    "# Test dengan matriks kecil\n",
    "A = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "B = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n",
    "\n",
    "manual_result = manual_matrix_multiply(A, B)\n",
    "torch_result = torch.matmul(A, B)\n",
    "\n",
    "assert torch.allclose(manual_result, torch_result), \"Manual multiplication incorrect\"\n",
    "print(\"✅ Advanced tensor operations completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
